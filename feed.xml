<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://junesjukim.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://junesjukim.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-28T13:49:00+00:00</updated><id>https://junesjukim.github.io/feed.xml</id><title type="html">blank</title><subtitle>June Sungjoo Kim is a Seoul-based Biosystems Engineering &amp; AI undergraduate at Seoul National University (SNU) (class of 2027). He is currently a research assistant in SNU&apos;s RLLAB, exploring Flow-Matching trajectory generation for offline RL, and a software engineer building accessibility apps for the visually impaired. His work spans robotic perception, on-device / edge AI, and bootstrapping tech startups that solve everyday problems for pets and people. </subtitle><entry><title type="html">Implicit Q-Learning (IQL) Explained</title><link href="https://junesjukim.github.io/studies/2024/iql-review/" rel="alternate" type="text/html" title="Implicit Q-Learning (IQL) Explained"/><published>2024-07-26T01:00:00+00:00</published><updated>2024-07-26T01:00:00+00:00</updated><id>https://junesjukim.github.io/studies/2024/iql-review</id><content type="html" xml:base="https://junesjukim.github.io/studies/2024/iql-review/"><![CDATA[<p>Implicit Q-Learning (IQL) is a simple yet powerful algorithm for offline reinforcement learning. Unlike traditional methods that rely on explicit policy constraints or complex value function regularization, IQL takes a different approach by learning a Q-function implicitly.</p> <p>The core idea is to treat the offline RL problem as a supervised learning problem. IQL uses <strong>Expectile Regression</strong> on the Q-function to extract a policy directly from the learned values. This avoids the need for out-of-distribution action queries, which is a major challenge in offline settings.</p> <p>By fitting one state-action value function and a simple state value function, IQL can derive a policy without ever needing to query the Q-function with unseen actions. This makes it a robust and effective method, demonstrating strong performance on standard offline RL benchmarks. Itâ€™s a great example of how reframing a problem can lead to simpler and more stable solutions.</p>]]></content><author><name></name></author><category term="paper reviews"/><summary type="html"><![CDATA[Implicit Q-Learning (IQL) is a simple yet powerful algorithm for offline reinforcement learning. Unlike traditional methods that rely on explicit policy constraints or complex value function regularization, IQL takes a different approach by learning a Q-function implicitly.]]></summary></entry></feed>