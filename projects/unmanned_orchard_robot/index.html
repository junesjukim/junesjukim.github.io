<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unmanned Orchard Robot | June Sungjoo Kim </title> <meta name="author" content="June Sungjoo Kim"> <meta name="description" content="Vision-Based Autonomous Guidance and Yield Monitoring"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://junesjukim.github.io/projects/unmanned_orchard_robot/"> <script src="/assets/js/theme.js?7f7c34135a72954c2d1252944009308c"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">June</span> Sungjoo Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repos </a> </li> <li class="nav-item "> <a class="nav-link" href="/studies/index.html">studies </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unmanned Orchard Robot</h1> <p class="post-description">Vision-Based Autonomous Guidance and Yield Monitoring</p> </header> <article> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/unmanned_orchard_hardware_1-480.webp 480w,/assets/img/unmanned_orchard_hardware_1-800.webp 800w,/assets/img/unmanned_orchard_hardware_1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/unmanned_orchard_hardware_1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Unmanned Orchard Robot Hardware" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The unmanned robot designed for autonomous orchard management and fruit monitoring. </div> <hr> <h3 id="1-overview"><strong>1. Overview</strong></h3> <p>This project introduces a <strong>ROS-based autonomous robot</strong> designed for modernizing orchard management. By leveraging computer vision and SLAM, the robot can navigate orchard rows, monitor fruit growth status, and detect diseases in real-time. This system aims to solve critical challenges in precision agriculture, such as labor shortages and the need for timely data-driven interventions. Our key achievement was the development of a fully integrated platform that successfully performed these tasks in a complex environment, ultimately winning the <strong>Grand Prize at the Agricultural Robot Competition</strong> hosted by the Rural Development Administration of Korea.</p> <div class="row justify-content-center"> <div class="col-sm-10 mt-3 mt-md-0"> <video width="100%" controls=""> <source src="/assets/img/unmanned_orchard_full_demo.mp4" type="video/mp4"></source> Your browser does not support the video tag. </video> </div> </div> <div class="caption"> Complete demonstration of the unmanned orchard robot in action, showcasing autonomous navigation and fruit detection capabilities. </div> <hr> <h3 id="2-the-challenge-precision-agriculture-in-orchards"><strong>2. The Challenge: Precision Agriculture in Orchards</strong></h3> <p>Orchard environments pose unique challenges for automation.</p> <ul> <li> <strong>GNSS-Denied Environment:</strong> Dense canopies block GPS signals, making standard navigation methods unreliable.</li> <li> <strong>Unstructured Terrain:</strong> Irregular row spacing and scattered obstacles require robust perception and dynamic path planning.</li> <li> <strong>Variable Conditions:</strong> Fluctuating light and weather conditions demand a vision system that is resilient to change.</li> </ul> <p>Our goal was to build a cost-effective robot that could reliably operate under these constraints using primarily vision and LiDAR sensors.</p> <hr> <h3 id="3-system-architecture--hardware"><strong>3. System Architecture &amp; Hardware</strong></h3> <p>The robot is built on a modular hardware and software architecture to ensure flexibility and robustness.</p> <div class="row justify-content-sm-center"> <div class="col-sm-5 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/unmanned_orchard_hardware_overview-480.webp 480w,/assets/img/unmanned_orchard_hardware_overview-800.webp 800w,/assets/img/unmanned_orchard_hardware_overview-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/unmanned_orchard_hardware_overview.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Hardware Architecture Overview" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-5 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/unmanned_orchard_algorithm_overview-480.webp 480w,/assets/img/unmanned_orchard_algorithm_overview-800.webp 800w,/assets/img/unmanned_orchard_algorithm_overview-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/unmanned_orchard_algorithm_overview.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Algorithmic System Overview" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> **Left:** Hardware architecture overview showing system components and their interconnections. **Right:** Algorithmic workflow demonstrating the integration of SLAM navigation and computer vision for autonomous orchard management. </div> <ul> <li> <strong>Hardware Stack</strong>: <ul> <li> <strong>Chassis</strong>: TurtleBot3 Burger</li> <li> <strong>Single Board Computer</strong>: NVIDIA Jetson Nano</li> <li> <strong>Primary Sensor (SLAM)</strong>: RPLiDAR A2M8 2D LiDAR</li> <li> <strong>Vision Sensors</strong>: 2 x Logitech C270 Webcams</li> <li> <strong>Controller</strong>: OpenCR 1.0 with Dynamixel motors</li> </ul> </li> <li> <strong>Software Stack</strong>: <ul> <li> <strong>OS</strong>: Ubuntu 18.04</li> <li> <strong>Framework</strong>: Robot Operating System (ROS1) Melodic</li> <li> <strong>Key Libraries</strong>: <code class="language-plaintext highlighter-rouge">GMapping</code> for SLAM, <code class="language-plaintext highlighter-rouge">PyTorch</code> for deep learning, <code class="language-plaintext highlighter-rouge">OpenCV</code> for image processing.</li> </ul> </li> </ul> <hr> <h3 id="4-dataset-collection--preparation-strategy"><strong>4. Dataset Collection &amp; Preparation Strategy</strong></h3> <p>To develop a robust fruit detection system, we implemented a systematic and comprehensive dataset preparation strategy that emphasized diversity and bias reduction.</p> <div class="row justify-content-sm-center"> <div class="col-sm-5 mt-3 mt-md-0"> <video width="100%" controls=""> <source src="/assets/img/unmanned_orchard_dataset_collection.mp4" type="video/mp4"></source> Your browser does not support the video tag. </video> </div> <div class="col-sm-5 mt-3 mt-md-0"> <video width="100%" controls=""> <source src="/assets/img/unmanned_orchard_dataset.mp4" type="video/mp4"></source> Your browser does not support the video tag. </video> </div> </div> <div class="caption"> **Left:** Dataset collection process showing systematic data capture from multiple angles and distances. **Right:** Overview of the diverse dataset including different fruit conditions and environmental scenarios. </div> <h4 id="41-initial-laboratory-dataset-793-images"><strong>4.1 Initial Laboratory Dataset (793 images)</strong></h4> <p>We first created a controlled dataset in our laboratory environment to establish baseline performance:</p> <ul> <li> <strong>Fruit Combinations</strong>: Healthy fruits (0-5), Diseased fruits (0-2)</li> <li> <strong>Distance Variations</strong>: 3 different camera-to-tree distances</li> <li> <strong>Angular Coverage</strong>: 10 different angles per setup</li> <li> <strong>Position Adjustments</strong>: Complete fruit repositioning for 2x variation</li> <li> <strong>Total Systematic Combinations</strong>: 6×3×3×10×2 = 1,080 planned images</li> </ul> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/unmanned_orchard_yolov5_mosaicaugmentation-480.webp 480w,/assets/img/unmanned_orchard_yolov5_mosaicaugmentation-800.webp 800w,/assets/img/unmanned_orchard_yolov5_mosaicaugmentation-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/unmanned_orchard_yolov5_mosaicaugmentation.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="YOLOv5 Mosaic Augmentation Example" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Example of YOLOv5's Mosaic Augmentation technique applied to our orchard dataset, combining multiple images to increase training robustness. </div> <h4 id="42-bias-reduction-strategies"><strong>4.2 Bias Reduction Strategies</strong></h4> <p>To address potential data biases, we implemented several corrective measures:</p> <ol> <li> <strong>Scale Bias Correction</strong>: Added close-up fruit images to counteract the bias toward small-scale fruits <ul> <li>Healthy fruits: 2, 1, 0 configurations</li> <li>Diseased fruits: 4, 3, 2, 1, 0 configurations</li> <li>2 images per configuration: 3×5×2 = 30 additional images</li> </ul> </li> </ol> <div class="row justify-content-sm-center"> <div class="col-sm-5 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/unmanned_orchard_scaled_dataset-480.webp 480w,/assets/img/unmanned_orchard_scaled_dataset-800.webp 800w,/assets/img/unmanned_orchard_scaled_dataset-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/unmanned_orchard_scaled_dataset.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Scale Bias Correction - Various Fruit Scales" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-5 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/unmanned_orchard_scaled_dataset_2-480.webp 480w,/assets/img/unmanned_orchard_scaled_dataset_2-800.webp 800w,/assets/img/unmanned_orchard_scaled_dataset_2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/unmanned_orchard_scaled_dataset_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Close-up Dataset Examples" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> **Left:** Examples of various fruit scales in the dataset showing different distances and sizes to reduce scale bias. **Right:** Close-up fruit images added to counteract small-scale bias, featuring different configurations of healthy and diseased fruits. </div> <ol> <li> <p><strong>Angular Diversity</strong>: 180-degree rotation captures with repositioning to handle non-frontal detection scenarios</p> </li> <li> <p><strong>Environmental Adaptation</strong>: Systematic variation in lighting conditions and camera settings</p> </li> </ol> <p><strong>Final Lab Dataset</strong>: 786 images with rich diversity and minimal bias</p> <h4 id="43-competition-environment-adaptation-345-images"><strong>4.3 Competition Environment Adaptation (345 images)</strong></h4> <ul> <li> <strong>Transfer Learning Strategy</strong>: Used lab dataset for pre-training, then fine-tuned with competition environment data</li> <li> <strong>Camera-Specific Tuning</strong>: Adapted to actual camera specifications, lighting, and field conditions</li> <li> <strong>Validation</strong>: Lab-only model (best.pt) outperformed mixed-data model, confirming our diversity-first approach</li> </ul> <hr> <h3 id="5-model-development--optimization"><strong>5. Model Development &amp; Optimization</strong></h3> <h4 id="51-model-selection--evolution"><strong>5.1 Model Selection &amp; Evolution</strong></h4> <div class="row justify-content-sm-center align-items-stretch"> <div class="col-sm-5 mt-3 mt-md-0 d-flex"> <div class="w-100"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/unmanned_orchard_train_result-480.webp 480w,/assets/img/unmanned_orchard_train_result-800.webp 800w,/assets/img/unmanned_orchard_train_result-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/unmanned_orchard_train_result.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Training Results Comparison" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="col-sm-5 mt-3 mt-md-0 d-flex"> <div class="w-100"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/unmanned_orchard_confidence_matrix-480.webp 480w,/assets/img/unmanned_orchard_confidence_matrix-800.webp 800w,/assets/img/unmanned_orchard_confidence_matrix-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/unmanned_orchard_confidence_matrix.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Model Confidence Analysis" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> **Left:** Comprehensive training results showing loss curves and performance metrics across different model iterations. **Right:** Model confidence analysis demonstrating prediction reliability and uncertainty quantification across different classes. </div> <p><strong>Model Evolution Process:</strong></p> <ol> <li> <strong>Initial Model</strong>: YOLOv5s with pretrained COCO weights</li> <li> <strong>Optimization</strong>: Transitioned to YOLOv5n for Jetson Nano compatibility</li> <li> <strong>Transfer Learning</strong>: Leveraged ImageNet and COCO pretrained weights over random initialization</li> <li> <strong>Edge Optimization</strong>: Model specifically tuned for real-time inference on edge hardware</li> </ol> <h4 id="52-training-strategy"><strong>5.2 Training Strategy</strong></h4> <ul> <li> <strong>Epochs</strong>: 200 epochs with best model checkpoint saving</li> <li> <strong>Early Stopping</strong>: Disabled to ensure complete training convergence</li> <li> <strong>Anchor Optimization</strong>: YOLOv5’s automatic anchor optimization using K-means and genetic algorithms</li> <li> <strong>Auto-Scaling</strong>: Automatic image size normalization for robust performance</li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/unmanned_orchard_confusion_matrix-480.webp 480w,/assets/img/unmanned_orchard_confusion_matrix-800.webp 800w,/assets/img/unmanned_orchard_confusion_matrix-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/unmanned_orchard_confusion_matrix.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Model Performance - Confusion Matrix" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Confusion matrix showing excellent classification performance across all three classes: Tree, Healthy Fruit (정상과), and Diseased Fruit (질병과). </div> <h4 id="53-technical-insights"><strong>5.3 Technical Insights</strong></h4> <ul> <li> <strong>Bounding Box Adaptation</strong>: Camera width changes required retraining due to target proportion variations</li> <li> <strong>Rotation Sensitivity</strong>: Simple rotation affected tree detection (vertical vs. horizontal orientation)</li> <li> <strong>Mosaic Augmentation</strong>: Enhanced model robustness through YOLOv5’s advanced data augmentation</li> </ul> <hr> <h3 id="6-core-technologies"><strong>6. Core Technologies</strong></h3> <h4 id="61-autonomous-navigation-with-slam"><strong>6.1 Autonomous Navigation with SLAM</strong></h4> <p>To navigate without GPS, the robot uses the <strong>GMapping SLAM</strong> algorithm. The 2D LiDAR sensor scans the environment to build a map of tree trunks and other obstacles. This map, combined with wheel odometry data from the Dynamixel motors, allows the robot to accurately determine its position and navigate autonomously along the orchard rows.</p> <h4 id="62-real-time-fruit-detection--classification"><strong>6.2 Real-time Fruit Detection &amp; Classification</strong></h4> <p>Our YOLOv5n model performs real-time detection with the following specifications:</p> <ul> <li> <strong>Classes</strong>: Tree, Healthy Fruit, Diseased Fruit</li> <li> <strong>Accuracy</strong>: 97% on test dataset</li> <li> <strong>Inference Speed</strong>: Optimized for Jetson Nano real-time processing</li> <li> <strong>Robustness</strong>: Handles various lighting conditions and viewing angles</li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm-5 mt-3 mt-md-0"> <video width="100%" controls=""> <source src="/assets/img/unmanned_orchard_ai_detection.mp4" type="video/mp4"></source> Your browser does not support the video tag. </video> </div> <div class="col-sm-5 mt-3 mt-md-0"> <video width="100%" controls=""> <source src="/assets/img/unmanned_orchard_ai_detection_2.mp4" type="video/mp4"></source> Your browser does not support the video tag. </video> </div> </div> <div class="caption"> Real-time demonstration of the YOLOv5n model identifying healthy and diseased fruits with high accuracy, showing bounding boxes and classification confidence scores optimized for edge computing. </div> <hr> <h3 id="7-results--competition-success"><strong>7. Results &amp; Competition Success</strong></h3> <p>The final integrated system was tested in a mock orchard environment. The robot successfully navigated the rows, detected all target trees, and created a position map of healthy and diseased fruits. The project’s success was recognized with the <strong>Grand Prize</strong> at the 60th-anniversary Agricultural Robot Competition.</p> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/unmanned_orchard_competition_result-480.webp 480w,/assets/img/unmanned_orchard_competition_result-800.webp 800w,/assets/img/unmanned_orchard_competition_result-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/unmanned_orchard_competition_result.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Competition Grand Prize Results" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Grand Prize award at the 60th Anniversary Agricultural Robot Competition hosted by the Rural Development Administration of Korea. </div> <div class="row justify-content-center"> <div class="col-sm-10 mt-3 mt-md-0"> <video width="100%" controls=""> <source src="/assets/img/unmanned_orchard_full_demo_2.mp4" type="video/mp4"></source> Your browser does not support the video tag. </video> </div> </div> <div class="caption"> Competition demonstration showing the complete autonomous mission including navigation, fruit detection, and mapping capabilities. </div> <p><strong>Key Achievements:</strong></p> <ul> <li>Successful autonomous navigation in GPS-denied environment</li> <li>97% accuracy in fruit and disease detection with systematic dataset preparation</li> <li>Real-time processing on edge computing platform (Jetson Nano)</li> <li>Robust performance under various lighting and weather conditions</li> <li>Advanced data preparation strategy with bias reduction techniques</li> <li> <strong>Grand Prize</strong> winner at national agricultural robotics competition</li> </ul> <hr> <h3 id="8-technical-contributions--lessons-learned"><strong>8. Technical Contributions &amp; Lessons Learned</strong></h3> <p><strong>Data Science Contributions:</strong></p> <ul> <li>Systematic dataset design methodology emphasizing diversity over quantity</li> <li>Bias identification and mitigation strategies for agricultural computer vision</li> <li>Transfer learning optimization for domain-specific applications</li> <li>Edge computing model optimization maintaining high accuracy</li> </ul> <p><strong>Engineering Insights:</strong></p> <ul> <li>Camera-specific retraining necessity for bounding box accuracy</li> <li>Importance of angular diversity in agricultural object detection</li> <li>Edge hardware constraints driving model architecture decisions</li> <li>Integration challenges between SLAM navigation and computer vision systems</li> </ul> <hr> <h3 id="9-conclusion--future-work"><strong>9. Conclusion &amp; Future Work</strong></h3> <p>This project successfully demonstrated the feasibility of a low-cost, vision-based robot for orchard automation with a particular emphasis on rigorous data preparation and model optimization for edge computing environments.</p> <p><strong>Technical Contributions:</strong></p> <ul> <li>Integration of SLAM navigation with computer vision for precision agriculture</li> <li>Edge-optimized deep learning model with systematic dataset preparation</li> <li>Robust system design for challenging outdoor environments</li> <li>Comprehensive bias reduction methodology for agricultural AI</li> </ul> <p><strong>Potential next steps include:</strong></p> <ul> <li>Testing and fine-tuning the system in real-world orchard environments</li> <li>Integrating robotic manipulation for automated harvesting based on detection results</li> <li>Improving long-term localization robustness with visual-inertial SLAM</li> <li>Expanding detection capabilities to multiple fruit varieties and disease types</li> <li>Scaling dataset preparation methodology for larger agricultural applications</li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 June Sungjoo Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>